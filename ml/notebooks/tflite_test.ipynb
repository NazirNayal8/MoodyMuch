{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "from data.affwild2_dataset import AffWild2ExprDataset\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4722, 0.3625, 0.3350]\n",
    "std = [0.2181, 0.1921, 0.1855]\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "skip = 0\n",
    "dataset = AffWild2ExprDataset(train=False, skip=skip, remove_mismatch=True,\n",
    "                                    compress=True, transform=transform_valid, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/nnayal17/.conda/envs/nazir_env/lib/python3.8/site-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    }
   ],
   "source": [
    "sample, label = dataset[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 112, 112]), 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e12026fc1f41439aa32674a3fc62d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15266.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_models/resnet18.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(data_len)):\n",
    "    sample, label = dataset[i]\n",
    "    \n",
    "    img = sample\n",
    "    img = img[None,:,:,:]\n",
    "    input_data = np.array(img, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    pred = np.argmax(output_data[0])\n",
    "    if pred == label:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Test Accuracy 60.7559282064719\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Test Accuracy {100 * correct / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Test Accuracy 66.23870037992926\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Test Accuracy {100 * correct / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04790531  0.41730937]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "#img = mpimg.imread('samples/test.jpg').transpose(2, 0, 1)\n",
    "img = sample.transpose(2, 0, 1)\n",
    "img = img[None,:,:,:]\n",
    "input_data = np.array(img, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /datasets/affwild2_latest/cropped_aligned_images/144/07804.jpg /scratch/users/nnayal17/moody_much/MoodyMuch/ml/samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /datasets/affwild2_latest/cropped_aligned_images/186/01265.jpg /scratch/users/nnayal17/moody_much/MoodyMuch/ml/samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.mobilenetv2 import mobilenetv2\n",
    "from torchvision.models import resnet18\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model = resnet18(num_classes=2)\n",
    "resnet_model.load_state_dict(torch.load(\"chosen_models/resnet18.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2307,  0.1517]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2307,  0.1517], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "resnet_model.eval()\n",
    "for i in range(data_len):\n",
    "    sample, label = dataset[i]\n",
    "    \n",
    "    out = resnet_model(torch.from_numpy(sample.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2))\n",
    "    pred = torch.argmax(out[0]).item()\n",
    "    if pred == label:\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 75.67797720424473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Model Accuracy: {100 * c / data_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv2_model = mobilenetv2()\n",
    "mobilenetv2_model.classifier = nn.Linear(mobilenetv2_model.classifier.in_features, 2)\n",
    "mobilenetv2_model.load_state_dict(torch.load(\"chosen_models/mobilenetv2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5372f5d52449e192f70977410fd328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15266.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "mobilenetv2_model.eval()\n",
    "for i in tqdm(range(data_len)):\n",
    "    sample, label = dataset[i]\n",
    "    \n",
    "    out = mobilenetv2_model(torch.from_numpy(sample.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2))\n",
    "    pred = torch.argmax(out[0]).item()\n",
    "    if pred == label:\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 76.47058823529412\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Model Accuracy: {100 * c / data_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nazir_env",
   "language": "python",
   "name": "nazir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
